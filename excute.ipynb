{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd087ea0c2bfc8d33363b62de134cfa12cc037bed7b7625ab24887ee8fe91891aef",
   "display_name": "Python 3.8.5 64-bit ('pytorch_gpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'BERTModel' from 'e:\\\\ML\\\\chatbot\\\\BERTChatBot_with_pretrained_model\\\\BERTModel.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import BERTModel\n",
    "import data_load_for_Transformer as dlft\n",
    "import Utility\n",
    "import os\n",
    "import torch\n",
    "import imp\n",
    "imp.reload(BERTModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Initiating the hyperparameters...\")\n",
    "# num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 128, 16\n",
    "# lr, num_epochs, device = 0.001, 200, Utility.try_gpu()\n",
    "# ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4\n",
    "# key_size, query_size, value_size = 32, 32, 32\n",
    "# norm_shape = [32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initiating the hyperparameters...\n"
     ]
    }
   ],
   "source": [
    "print(\"Initiating the hyperparameters...\")\n",
    "num_hiddens, num_layers, dropout, batch_size, num_steps = 128, 2, 0.1, 384, 16\n",
    "lr, num_epochs, device = 0.0001, 20, Utility.try_gpu()\n",
    "ffn_num_input, ffn_num_hiddens, num_heads = 128, 128, 8\n",
    "key_size, query_size, value_size = 128, 128, 128\n",
    "norm_shape = [128]\n",
    "using_bias = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building the vocabulary...\n['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '你', '我', '的', '了', '是']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1034, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "print(\"Building the vocabulary...\")\n",
    "train_iter, vocab = dlft.load_data_xhj_for_Transformer(batch_size, num_steps,load=True)\n",
    "len(train_iter), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rebuilding the Model...\nCan not load the model with error: [Errno 2] No such file or directory: 'model_data/model_transformer.pt'\nReady to working...\n"
     ]
    }
   ],
   "source": [
    "print(\"Rebuilding the Model...\")\n",
    "encoder = BERTModel.TransformerEncoder(\n",
    "    len(vocab), key_size, query_size, value_size, num_hiddens,\n",
    "    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "    num_layers, dropout, using_bias)\n",
    "decoder = BERTModel.TransformerDecoder(\n",
    "    len(vocab), key_size, query_size, value_size, num_hiddens,\n",
    "    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "    num_layers, dropout)\n",
    "net = BERTModel.EncoderDecoder(encoder, decoder)\n",
    "try:\n",
    "    checkpoint_prefix = os.path.join(\"model_data/model_transformer.pt\")\n",
    "    checkpoint = torch.load(checkpoint_prefix)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "except Exception as e:\n",
    "    print(\"Can not load the model with error:\", e)\n",
    "\n",
    "print(\"Ready to working...\")\n",
    "def predict(src_sentence):\n",
    "    return BERTModel.predict_seq2seq(net, src_sentence, vocab, vocab, num_steps,\n",
    "                    device, save_attention_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['<unk>', '<pad>', 'mask2', '<bos>', '<eos>', '你', '我', '的', '了', '是']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.to_tokens([0,1,2,3,4,5,6,7,8,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6605c3163911>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBERTModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_seq2seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\ML\\chatbot\\BERTChatBot_torch\\BERTModel.py\u001b[0m in \u001b[0;36mtrain_seq2seq\u001b[1;34m(net, data_iter, lr, num_epochs, batch_size, tgt_vocab, device)\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[0mY_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_valid_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_valid_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Make the loss scalar for `backward`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m             \u001b[1;31m# Utility.grad_clipping(net, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BERTModel.train_seq2seq(net, train_iter, lr, num_epochs, batch_size, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTModel.train_seq2seq(net, train_iter, lr, num_epochs, batch_size, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "src ['今', '天', '天', '气', '不', '错']\nsrc_tokens= [7286, 627, 627, 1282, 11, 163, 3]\ntp_src_tokens [7286, 627, 627, 1282, 11, 163, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nenc_outputs tensor([[[-4.7648e-02,  2.1777e-01,  6.3488e-02,  2.8417e-02, -9.2419e-02,\n          -2.5271e-02,  1.0760e-01,  1.6060e-01,  3.0683e-02,  5.1947e-02,\n          -7.5754e-02, -2.2376e-02, -1.3619e-01,  6.1030e-02,  1.0808e-01,\n           2.0210e-02, -2.1162e-02,  2.7419e-04,  1.8149e-01, -2.0128e-03,\n          -7.8765e-02, -1.1256e-01, -3.5420e-03, -1.0250e-02,  1.3250e-01,\n           2.4457e-02,  7.9618e-02,  1.4568e-01, -2.8701e-02, -1.0381e-01,\n          -8.9158e-02, -1.2742e-02],\n         [ 3.0389e-03,  2.0170e-01,  4.4093e-02,  2.4734e-02, -5.9368e-02,\n          -7.8783e-02,  1.2622e-01,  1.2904e-01,  4.0874e-02,  5.1486e-02,\n          -7.6375e-02, -2.7593e-02, -1.6642e-01,  5.8921e-02,  1.3456e-01,\n           2.8648e-02, -2.9015e-02, -3.2208e-02,  8.6804e-02,  4.3393e-02,\n          -6.8488e-02, -1.1664e-01,  7.5889e-04,  6.0728e-04,  9.8424e-02,\n           4.1310e-02,  1.5620e-01,  1.8302e-01, -3.2575e-02, -1.0584e-01,\n          -9.4559e-02, -4.1287e-02],\n         [ 3.3511e-03,  2.0221e-01,  4.4041e-02,  2.4699e-02, -5.8986e-02,\n          -7.9057e-02,  1.2616e-01,  1.2871e-01,  4.0881e-02,  5.1483e-02,\n          -7.6315e-02, -2.7579e-02, -1.6661e-01,  5.8876e-02,  1.3465e-01,\n           2.8720e-02, -2.9124e-02, -3.2348e-02,  8.6218e-02,  4.3867e-02,\n          -6.8450e-02, -1.1660e-01,  7.3402e-04,  7.8426e-04,  9.8145e-02,\n           4.1428e-02,  1.5654e-01,  1.8316e-01, -3.2628e-02, -1.0583e-01,\n          -9.4380e-02, -4.1440e-02],\n         [-3.9345e-02,  5.7612e-02,  5.3615e-02,  1.2747e-02, -6.7519e-02,\n          -1.1178e-01,  1.4566e-01,  1.8707e-01,  5.1209e-02,  5.1728e-02,\n          -1.0514e-01, -5.0791e-02, -1.7141e-01,  6.5673e-02,  1.4472e-01,\n           2.0301e-02, -1.2813e-02, -3.2155e-02,  1.5496e-01, -1.3507e-02,\n          -4.9823e-02, -1.2949e-01,  2.4734e-02, -2.3089e-02,  1.2042e-01,\n           2.0584e-02,  1.5193e-01,  1.9717e-01, -2.7452e-02, -1.0195e-01,\n          -1.4836e-01, -3.2067e-02],\n         [-8.1242e-02,  2.5136e-02,  8.2260e-02,  1.1421e-02, -8.9493e-02,\n          -6.6909e-02,  1.3486e-01,  2.2012e-01,  5.6560e-02,  5.2085e-02,\n          -1.0856e-01, -5.5541e-02, -1.6069e-01,  5.6242e-02,  1.1932e-01,\n           1.5068e-02,  2.1193e-04, -1.1091e-02,  2.8095e-01, -2.9702e-02,\n          -3.7818e-02, -1.4422e-01,  4.4621e-02, -2.2264e-02,  1.3561e-01,\n           1.2143e-03,  9.9339e-02,  1.8054e-01, -1.9939e-02, -8.8044e-02,\n          -1.6344e-01, -1.1999e-02],\n         [-1.0396e-01, -9.5095e-04,  7.0704e-02,  7.7148e-03, -1.1297e-01,\n           4.4195e-02,  1.2892e-01,  1.7862e-01,  4.4210e-02,  5.2230e-02,\n          -6.7898e-02, -3.2639e-02, -6.1562e-02,  2.6476e-02,  7.8144e-02,\n           2.6353e-02,  6.2236e-03, -1.8843e-02,  1.8846e-01, -6.8539e-02,\n          -2.0297e-02, -8.5872e-02,  3.1618e-02, -2.5039e-02,  1.1437e-01,\n           1.3104e-02,  2.7061e-02,  1.6544e-01, -2.1310e-02, -7.8847e-02,\n          -1.4773e-01, -3.8384e-03],\n         [-1.2083e-02, -3.1306e-02,  8.3287e-02, -2.9687e-02, -5.4419e-02,\n           1.4393e-01,  5.7890e-02,  1.0947e-02,  1.9554e-02,  5.4188e-02,\n           2.6157e-02,  5.2948e-02,  1.7096e-01, -9.5250e-02, -4.6700e-02,\n           7.5917e-02, -2.0001e-02, -1.3174e-01, -4.9334e-02, -2.7778e-02,\n           1.1367e-01,  2.2491e-02, -9.2111e-04,  1.3956e-02, -9.3538e-02,\n           6.2343e-02, -1.7301e-01, -1.2774e-02, -3.5514e-02, -1.3185e-02,\n           6.3173e-03,  1.7370e-02],\n         [-9.4911e-02, -3.8397e-02,  5.5537e-02,  3.6488e-04, -1.4465e-01,\n           9.9887e-02,  1.1653e-01,  1.4661e-01,  3.2085e-02,  5.2786e-02,\n          -4.2533e-02, -1.3793e-02,  6.0177e-03,  1.1241e-03,  4.8669e-02,\n           3.2544e-02,  7.0973e-03, -3.7627e-02,  1.0632e-01, -9.3001e-02,\n          -1.6763e-02, -3.4696e-02,  1.1966e-02, -2.3403e-02,  9.9208e-02,\n           1.3601e-02, -2.1943e-02,  1.1974e-01, -2.3186e-02, -7.2202e-02,\n          -1.2072e-01,  5.4548e-03],\n         [-9.8286e-02, -3.4253e-02,  5.6836e-02,  1.5310e-03, -1.4390e-01,\n           9.1200e-02,  1.1968e-01,  1.5407e-01,  3.4157e-02,  5.2691e-02,\n          -4.7292e-02, -1.7603e-02, -5.3150e-03,  6.6102e-03,  5.5657e-02,\n           3.0871e-02,  7.6347e-03, -3.3674e-02,  1.1913e-01, -9.2262e-02,\n          -1.8543e-02, -4.1667e-02,  1.4639e-02, -2.4805e-02,  1.0296e-01,\n           1.2011e-02, -1.2278e-02,  1.2825e-01, -2.3101e-02, -7.4411e-02,\n          -1.2557e-01,  3.6441e-03],\n         [-1.0029e-01, -3.1332e-02,  5.7870e-02,  2.2810e-03, -1.4282e-01,\n           8.5432e-02,  1.2141e-01,  1.5853e-01,  3.5425e-02,  5.2630e-02,\n          -5.0229e-02, -1.9923e-02, -1.2386e-02,  9.9146e-03,  5.9809e-02,\n           2.9918e-02,  7.8107e-03, -3.1188e-02,  1.2736e-01, -9.1282e-02,\n          -1.9359e-02, -4.6258e-02,  1.6421e-02, -2.5519e-02,  1.0520e-01,\n           1.1196e-02, -6.4968e-03,  1.3350e-01, -2.3054e-02, -7.5674e-02,\n          -1.2845e-01,  2.4943e-03],\n         [-9.8231e-02, -3.3498e-02,  5.7045e-02,  1.7801e-03, -1.4424e-01,\n           8.9993e-02,  1.2001e-01,  1.5488e-01,  3.4487e-02,  5.2678e-02,\n          -4.7794e-02, -1.8000e-02, -6.7956e-03,  7.2789e-03,  5.6432e-02,\n           3.0703e-02,  7.6597e-03, -3.3074e-02,  1.2047e-01, -9.1967e-02,\n          -1.8736e-02, -4.2986e-02,  1.5098e-02, -2.4859e-02,  1.0358e-01,\n           1.2041e-02, -1.0800e-02,  1.2928e-01, -2.3056e-02, -7.4615e-02,\n          -1.2649e-01,  3.3467e-03],\n         [-9.3396e-02, -3.8472e-02,  5.5371e-02,  4.1310e-04, -1.4576e-01,\n           1.0084e-01,  1.1605e-01,  1.4533e-01,  3.1933e-02,  5.2797e-02,\n          -4.1744e-02, -1.3141e-02,  7.3723e-03,  4.1596e-04,  4.7599e-02,\n           3.2835e-02,  6.9349e-03, -3.7916e-02,  1.0406e-01, -9.2806e-02,\n          -1.6525e-02, -3.4546e-02,  1.1799e-02, -2.3008e-02,  9.9003e-02,\n           1.4219e-02, -2.2655e-02,  1.1862e-01, -2.3146e-02, -7.1819e-02,\n          -1.2063e-01,  5.5184e-03],\n         [-9.0493e-02, -4.1572e-02,  5.4600e-02, -5.4140e-04, -1.4582e-01,\n           1.0714e-01,  1.1352e-01,  1.3944e-01,  3.0253e-02,  5.2881e-02,\n          -3.7778e-02, -1.0169e-02,  1.6780e-02, -3.8895e-03,  4.1955e-02,\n           3.4161e-02,  6.0308e-03, -4.0937e-02,  9.4318e-02, -9.2035e-02,\n          -1.4586e-02, -2.9689e-02,  1.0012e-02, -2.1616e-02,  9.6019e-02,\n           1.5533e-02, -3.0730e-02,  1.1153e-01, -2.3231e-02, -7.0127e-02,\n          -1.1672e-01,  6.9416e-03],\n         [-9.2298e-02, -4.0059e-02,  5.5019e-02, -5.6730e-05, -1.4615e-01,\n           1.0356e-01,  1.1511e-01,  1.4295e-01,  3.1211e-02,  5.2831e-02,\n          -4.0117e-02, -1.1903e-02,  1.1328e-02, -1.3695e-03,  4.5277e-02,\n           3.3373e-02,  6.6077e-03, -3.9187e-02,  1.0006e-01, -9.2718e-02,\n          -1.5750e-02, -3.2252e-02,  1.1017e-02, -2.2525e-02,  9.7624e-02,\n           1.4690e-02, -2.6156e-02,  1.1563e-01, -2.3194e-02, -7.1167e-02,\n          -1.1887e-01,  6.1455e-03],\n         [-9.8770e-02, -3.3729e-02,  5.7132e-02,  1.7053e-03, -1.4489e-01,\n           8.9315e-02,  1.2049e-01,  1.5568e-01,  3.4606e-02,  5.2670e-02,\n          -4.8229e-02, -1.8328e-02, -7.6089e-03,  7.7646e-03,  5.7063e-02,\n           3.0523e-02,  7.7318e-03, -3.2743e-02,  1.2166e-01, -9.2168e-02,\n          -1.8861e-02, -4.3114e-02,  1.5299e-02, -2.5144e-02,  1.0363e-01,\n           1.1761e-02, -1.0323e-02,  1.2979e-01, -2.3078e-02, -7.4918e-02,\n          -1.2659e-01,  3.2614e-03],\n         [-1.0467e-01, -2.6206e-02,  6.0040e-02,  3.3874e-03, -1.4009e-01,\n           7.4124e-02,  1.2457e-01,  1.6682e-01,  3.7618e-02,  5.2508e-02,\n          -5.5946e-02, -2.4286e-02, -2.5152e-02,  1.5977e-02,  6.7386e-02,\n           2.8375e-02,  7.7382e-03, -2.6660e-02,  1.4336e-01, -8.8738e-02,\n          -2.0140e-02, -5.3960e-02,  1.9871e-02, -2.6955e-02,  1.0842e-01,\n           9.5293e-03,  3.0091e-03,  1.4286e-01, -2.3085e-02, -7.8048e-02,\n          -1.3253e-01,  4.6809e-04]]], device='cuda:0',\n       grad_fn=<NativeLayerNormBackward>)\ndec_state [tensor([[[-4.7648e-02,  2.1777e-01,  6.3488e-02,  2.8417e-02, -9.2419e-02,\n          -2.5271e-02,  1.0760e-01,  1.6060e-01,  3.0683e-02,  5.1947e-02,\n          -7.5754e-02, -2.2376e-02, -1.3619e-01,  6.1030e-02,  1.0808e-01,\n           2.0210e-02, -2.1162e-02,  2.7419e-04,  1.8149e-01, -2.0128e-03,\n          -7.8765e-02, -1.1256e-01, -3.5420e-03, -1.0250e-02,  1.3250e-01,\n           2.4457e-02,  7.9618e-02,  1.4568e-01, -2.8701e-02, -1.0381e-01,\n          -8.9158e-02, -1.2742e-02],\n         [ 3.0389e-03,  2.0170e-01,  4.4093e-02,  2.4734e-02, -5.9368e-02,\n          -7.8783e-02,  1.2622e-01,  1.2904e-01,  4.0874e-02,  5.1486e-02,\n          -7.6375e-02, -2.7593e-02, -1.6642e-01,  5.8921e-02,  1.3456e-01,\n           2.8648e-02, -2.9015e-02, -3.2208e-02,  8.6804e-02,  4.3393e-02,\n          -6.8488e-02, -1.1664e-01,  7.5889e-04,  6.0728e-04,  9.8424e-02,\n           4.1310e-02,  1.5620e-01,  1.8302e-01, -3.2575e-02, -1.0584e-01,\n          -9.4559e-02, -4.1287e-02],\n         [ 3.3511e-03,  2.0221e-01,  4.4041e-02,  2.4699e-02, -5.8986e-02,\n          -7.9057e-02,  1.2616e-01,  1.2871e-01,  4.0881e-02,  5.1483e-02,\n          -7.6315e-02, -2.7579e-02, -1.6661e-01,  5.8876e-02,  1.3465e-01,\n           2.8720e-02, -2.9124e-02, -3.2348e-02,  8.6218e-02,  4.3867e-02,\n          -6.8450e-02, -1.1660e-01,  7.3402e-04,  7.8426e-04,  9.8145e-02,\n           4.1428e-02,  1.5654e-01,  1.8316e-01, -3.2628e-02, -1.0583e-01,\n          -9.4380e-02, -4.1440e-02],\n         [-3.9345e-02,  5.7612e-02,  5.3615e-02,  1.2747e-02, -6.7519e-02,\n          -1.1178e-01,  1.4566e-01,  1.8707e-01,  5.1209e-02,  5.1728e-02,\n          -1.0514e-01, -5.0791e-02, -1.7141e-01,  6.5673e-02,  1.4472e-01,\n           2.0301e-02, -1.2813e-02, -3.2155e-02,  1.5496e-01, -1.3507e-02,\n          -4.9823e-02, -1.2949e-01,  2.4734e-02, -2.3089e-02,  1.2042e-01,\n           2.0584e-02,  1.5193e-01,  1.9717e-01, -2.7452e-02, -1.0195e-01,\n          -1.4836e-01, -3.2067e-02],\n         [-8.1242e-02,  2.5136e-02,  8.2260e-02,  1.1421e-02, -8.9493e-02,\n          -6.6909e-02,  1.3486e-01,  2.2012e-01,  5.6560e-02,  5.2085e-02,\n          -1.0856e-01, -5.5541e-02, -1.6069e-01,  5.6242e-02,  1.1932e-01,\n           1.5068e-02,  2.1193e-04, -1.1091e-02,  2.8095e-01, -2.9702e-02,\n          -3.7818e-02, -1.4422e-01,  4.4621e-02, -2.2264e-02,  1.3561e-01,\n           1.2143e-03,  9.9339e-02,  1.8054e-01, -1.9939e-02, -8.8044e-02,\n          -1.6344e-01, -1.1999e-02],\n         [-1.0396e-01, -9.5095e-04,  7.0704e-02,  7.7148e-03, -1.1297e-01,\n           4.4195e-02,  1.2892e-01,  1.7862e-01,  4.4210e-02,  5.2230e-02,\n          -6.7898e-02, -3.2639e-02, -6.1562e-02,  2.6476e-02,  7.8144e-02,\n           2.6353e-02,  6.2236e-03, -1.8843e-02,  1.8846e-01, -6.8539e-02,\n          -2.0297e-02, -8.5872e-02,  3.1618e-02, -2.5039e-02,  1.1437e-01,\n           1.3104e-02,  2.7061e-02,  1.6544e-01, -2.1310e-02, -7.8847e-02,\n          -1.4773e-01, -3.8384e-03],\n         [-1.2083e-02, -3.1306e-02,  8.3287e-02, -2.9687e-02, -5.4419e-02,\n           1.4393e-01,  5.7890e-02,  1.0947e-02,  1.9554e-02,  5.4188e-02,\n           2.6157e-02,  5.2948e-02,  1.7096e-01, -9.5250e-02, -4.6700e-02,\n           7.5917e-02, -2.0001e-02, -1.3174e-01, -4.9334e-02, -2.7778e-02,\n           1.1367e-01,  2.2491e-02, -9.2111e-04,  1.3956e-02, -9.3538e-02,\n           6.2343e-02, -1.7301e-01, -1.2774e-02, -3.5514e-02, -1.3185e-02,\n           6.3173e-03,  1.7370e-02],\n         [-9.4911e-02, -3.8397e-02,  5.5537e-02,  3.6488e-04, -1.4465e-01,\n           9.9887e-02,  1.1653e-01,  1.4661e-01,  3.2085e-02,  5.2786e-02,\n          -4.2533e-02, -1.3793e-02,  6.0177e-03,  1.1241e-03,  4.8669e-02,\n           3.2544e-02,  7.0973e-03, -3.7627e-02,  1.0632e-01, -9.3001e-02,\n          -1.6763e-02, -3.4696e-02,  1.1966e-02, -2.3403e-02,  9.9208e-02,\n           1.3601e-02, -2.1943e-02,  1.1974e-01, -2.3186e-02, -7.2202e-02,\n          -1.2072e-01,  5.4548e-03],\n         [-9.8286e-02, -3.4253e-02,  5.6836e-02,  1.5310e-03, -1.4390e-01,\n           9.1200e-02,  1.1968e-01,  1.5407e-01,  3.4157e-02,  5.2691e-02,\n          -4.7292e-02, -1.7603e-02, -5.3150e-03,  6.6102e-03,  5.5657e-02,\n           3.0871e-02,  7.6347e-03, -3.3674e-02,  1.1913e-01, -9.2262e-02,\n          -1.8543e-02, -4.1667e-02,  1.4639e-02, -2.4805e-02,  1.0296e-01,\n           1.2011e-02, -1.2278e-02,  1.2825e-01, -2.3101e-02, -7.4411e-02,\n          -1.2557e-01,  3.6441e-03],\n         [-1.0029e-01, -3.1332e-02,  5.7870e-02,  2.2810e-03, -1.4282e-01,\n           8.5432e-02,  1.2141e-01,  1.5853e-01,  3.5425e-02,  5.2630e-02,\n          -5.0229e-02, -1.9923e-02, -1.2386e-02,  9.9146e-03,  5.9809e-02,\n           2.9918e-02,  7.8107e-03, -3.1188e-02,  1.2736e-01, -9.1282e-02,\n          -1.9359e-02, -4.6258e-02,  1.6421e-02, -2.5519e-02,  1.0520e-01,\n           1.1196e-02, -6.4968e-03,  1.3350e-01, -2.3054e-02, -7.5674e-02,\n          -1.2845e-01,  2.4943e-03],\n         [-9.8231e-02, -3.3498e-02,  5.7045e-02,  1.7801e-03, -1.4424e-01,\n           8.9993e-02,  1.2001e-01,  1.5488e-01,  3.4487e-02,  5.2678e-02,\n          -4.7794e-02, -1.8000e-02, -6.7956e-03,  7.2789e-03,  5.6432e-02,\n           3.0703e-02,  7.6597e-03, -3.3074e-02,  1.2047e-01, -9.1967e-02,\n          -1.8736e-02, -4.2986e-02,  1.5098e-02, -2.4859e-02,  1.0358e-01,\n           1.2041e-02, -1.0800e-02,  1.2928e-01, -2.3056e-02, -7.4615e-02,\n          -1.2649e-01,  3.3467e-03],\n         [-9.3396e-02, -3.8472e-02,  5.5371e-02,  4.1310e-04, -1.4576e-01,\n           1.0084e-01,  1.1605e-01,  1.4533e-01,  3.1933e-02,  5.2797e-02,\n          -4.1744e-02, -1.3141e-02,  7.3723e-03,  4.1596e-04,  4.7599e-02,\n           3.2835e-02,  6.9349e-03, -3.7916e-02,  1.0406e-01, -9.2806e-02,\n          -1.6525e-02, -3.4546e-02,  1.1799e-02, -2.3008e-02,  9.9003e-02,\n           1.4219e-02, -2.2655e-02,  1.1862e-01, -2.3146e-02, -7.1819e-02,\n          -1.2063e-01,  5.5184e-03],\n         [-9.0493e-02, -4.1572e-02,  5.4600e-02, -5.4140e-04, -1.4582e-01,\n           1.0714e-01,  1.1352e-01,  1.3944e-01,  3.0253e-02,  5.2881e-02,\n          -3.7778e-02, -1.0169e-02,  1.6780e-02, -3.8895e-03,  4.1955e-02,\n           3.4161e-02,  6.0308e-03, -4.0937e-02,  9.4318e-02, -9.2035e-02,\n          -1.4586e-02, -2.9689e-02,  1.0012e-02, -2.1616e-02,  9.6019e-02,\n           1.5533e-02, -3.0730e-02,  1.1153e-01, -2.3231e-02, -7.0127e-02,\n          -1.1672e-01,  6.9416e-03],\n         [-9.2298e-02, -4.0059e-02,  5.5019e-02, -5.6730e-05, -1.4615e-01,\n           1.0356e-01,  1.1511e-01,  1.4295e-01,  3.1211e-02,  5.2831e-02,\n          -4.0117e-02, -1.1903e-02,  1.1328e-02, -1.3695e-03,  4.5277e-02,\n           3.3373e-02,  6.6077e-03, -3.9187e-02,  1.0006e-01, -9.2718e-02,\n          -1.5750e-02, -3.2252e-02,  1.1017e-02, -2.2525e-02,  9.7624e-02,\n           1.4690e-02, -2.6156e-02,  1.1563e-01, -2.3194e-02, -7.1167e-02,\n          -1.1887e-01,  6.1455e-03],\n         [-9.8770e-02, -3.3729e-02,  5.7132e-02,  1.7053e-03, -1.4489e-01,\n           8.9315e-02,  1.2049e-01,  1.5568e-01,  3.4606e-02,  5.2670e-02,\n          -4.8229e-02, -1.8328e-02, -7.6089e-03,  7.7646e-03,  5.7063e-02,\n           3.0523e-02,  7.7318e-03, -3.2743e-02,  1.2166e-01, -9.2168e-02,\n          -1.8861e-02, -4.3114e-02,  1.5299e-02, -2.5144e-02,  1.0363e-01,\n           1.1761e-02, -1.0323e-02,  1.2979e-01, -2.3078e-02, -7.4918e-02,\n          -1.2659e-01,  3.2614e-03],\n         [-1.0467e-01, -2.6206e-02,  6.0040e-02,  3.3874e-03, -1.4009e-01,\n           7.4124e-02,  1.2457e-01,  1.6682e-01,  3.7618e-02,  5.2508e-02,\n          -5.5946e-02, -2.4286e-02, -2.5152e-02,  1.5977e-02,  6.7386e-02,\n           2.8375e-02,  7.7382e-03, -2.6660e-02,  1.4336e-01, -8.8738e-02,\n          -2.0140e-02, -5.3960e-02,  1.9871e-02, -2.6955e-02,  1.0842e-01,\n           9.5293e-03,  3.0091e-03,  1.4286e-01, -2.3085e-02, -7.8048e-02,\n          -1.3253e-01,  4.6809e-04]]], device='cuda:0',\n       grad_fn=<NativeLayerNormBackward>), tensor([7], device='cuda:0'), [None, None]]\nY tensor([[[-24.1829, -24.1959, -24.1941,  -3.4378,  -1.2081,  -0.7633,  -5.3796,\n           -6.8449,  -2.3426,   0.0246]]], device='cuda:0',\n       grad_fn=<SliceBackward>) tensor([[9]], device='cuda:0') torch.return_types.max(\nvalues=tensor([[0.0246]], device='cuda:0', grad_fn=<MaxBackward0>),\nindices=tensor([[9]], device='cuda:0'))\nY tensor([[[-23.7879, -23.7379, -23.7095,   6.9263,  -1.6988,  -0.4357,  -2.1626,\n           -1.2156,  -4.0891,   6.7691]]], device='cuda:0',\n       grad_fn=<SliceBackward>) tensor([[3]], device='cuda:0') torch.return_types.max(\nvalues=tensor([[6.9263]], device='cuda:0', grad_fn=<MaxBackward0>),\nindices=tensor([[3]], device='cuda:0'))\n[9]\n['=']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'='"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "predict(\"今天天气不错\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "src ['你', '叫', '什', '么', '名', '字']\nsrc_tokens= [4, 65, 4423, 21, 2622, 964, 3]\ntp_src_tokens [4, 65, 4423, 21, 2622, 964, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nenc_outputs tensor([[[ 0.1969,  0.1436, -0.0772,  0.0018, -0.0761, -0.0181,  0.1787,\n          -0.0380,  0.0319,  0.0525, -0.0316, -0.0139, -0.0655,  0.0068,\n           0.1188,  0.0563, -0.0451, -0.0943, -0.3334,  0.1194, -0.0459,\n           0.0010, -0.0121, -0.0086, -0.0243,  0.0828,  0.2981,  0.1206,\n          -0.0396, -0.0989, -0.0042, -0.0949],\n         [ 0.1498,  0.0338, -0.0677, -0.0004, -0.1021,  0.0237,  0.1708,\n          -0.0302,  0.0287,  0.0527, -0.0496, -0.0226, -0.0528,  0.0102,\n           0.1071,  0.0491, -0.0330, -0.0859, -0.2743,  0.0435, -0.0233,\n           0.0030,  0.0099, -0.0177,  0.0058,  0.0756,  0.2667,  0.1455,\n          -0.0350, -0.0856, -0.0374, -0.0927],\n         [ 0.1842,  0.1141, -0.0724,  0.0030, -0.0875, -0.0235,  0.1857,\n          -0.0254,  0.0351,  0.0523, -0.0399, -0.0177, -0.0716,  0.0188,\n           0.1209,  0.0540, -0.0406, -0.0921, -0.2964,  0.0906, -0.0504,\n          -0.0010, -0.0097, -0.0137, -0.0018,  0.0815,  0.2924,  0.1352,\n          -0.0397, -0.1003, -0.0247, -0.0963],\n         [ 0.1186,  0.0026, -0.0548, -0.0019, -0.1288,  0.0450,  0.1726,\n          -0.0112,  0.0318,  0.0527, -0.0537, -0.0211, -0.0469,  0.0188,\n           0.0927,  0.0484, -0.0278, -0.0841, -0.2237,  0.0056, -0.0311,\n           0.0059,  0.0131, -0.0196,  0.0433,  0.0692,  0.2218,  0.1435,\n          -0.0349, -0.0835, -0.0613, -0.0834],\n         [ 0.1811,  0.1305, -0.0720,  0.0037, -0.0814, -0.0175,  0.1828,\n          -0.0278,  0.0345,  0.0524, -0.0386, -0.0164, -0.0701,  0.0153,\n           0.1193,  0.0542, -0.0420, -0.0921, -0.3075,  0.0975, -0.0491,\n          -0.0007, -0.0098, -0.0117, -0.0088,  0.0813,  0.2895,  0.1253,\n          -0.0407, -0.0995, -0.0196, -0.0947],\n         [ 0.1815,  0.1183, -0.0709,  0.0033, -0.0933, -0.0294,  0.1879,\n          -0.0221,  0.0361,  0.0522, -0.0416, -0.0181, -0.0749,  0.0216,\n           0.1229,  0.0532, -0.0405, -0.0860, -0.2905,  0.0866, -0.0505,\n          -0.0015, -0.0091, -0.0146,  0.0011,  0.0815,  0.2933,  0.1344,\n          -0.0406, -0.1016, -0.0262, -0.0981],\n         [ 0.0085, -0.0508,  0.0771, -0.0287, -0.0619,  0.1472,  0.0590,\n           0.0087,  0.0132,  0.0543,  0.0228,  0.0525,  0.1699, -0.0931,\n          -0.0461,  0.0709, -0.0215, -0.1456, -0.0837, -0.0194,  0.1083,\n           0.0187, -0.0065,  0.0126, -0.0830,  0.0635, -0.1613, -0.0177,\n          -0.0338, -0.0112,  0.0178,  0.0279],\n         [ 0.0215, -0.0888,  0.0315, -0.0158, -0.1427,  0.1554,  0.0900,\n           0.0274,  0.0100,  0.0543, -0.0109,  0.0204,  0.0770, -0.0485,\n          -0.0227,  0.0464, -0.0087, -0.0959, -0.0692, -0.0517,  0.0027,\n          -0.0030, -0.0107,  0.0005,  0.0500,  0.0450, -0.0654,  0.0298,\n          -0.0288, -0.0351, -0.0468,  0.0251],\n         [ 0.0145, -0.0796,  0.0168, -0.0112, -0.1801,  0.1423,  0.1122,\n           0.0490,  0.0138,  0.0539, -0.0267,  0.0074,  0.0373, -0.0197,\n           0.0072,  0.0374, -0.0081, -0.0714, -0.0717, -0.0787, -0.0211,\n          -0.0179, -0.0073, -0.0092,  0.0807,  0.0364, -0.0118,  0.0609,\n          -0.0269, -0.0517, -0.0693,  0.0102],\n         [ 0.0186, -0.0755,  0.0117, -0.0101, -0.1828,  0.1380,  0.1182,\n           0.0484,  0.0154,  0.0538, -0.0290,  0.0048,  0.0304, -0.0152,\n           0.0149,  0.0371, -0.0084, -0.0702, -0.0782, -0.0776, -0.0235,\n          -0.0175, -0.0060, -0.0109,  0.0815,  0.0369,  0.0051,  0.0693,\n          -0.0273, -0.0547, -0.0710,  0.0042],\n         [ 0.0176, -0.0764,  0.0128, -0.0103, -0.1826,  0.1388,  0.1170,\n           0.0487,  0.0151,  0.0538, -0.0285,  0.0053,  0.0318, -0.0160,\n           0.0134,  0.0372, -0.0083, -0.0704, -0.0767, -0.0779, -0.0230,\n          -0.0177, -0.0062, -0.0106,  0.0815,  0.0368,  0.0017,  0.0676,\n          -0.0272, -0.0542, -0.0708,  0.0054],\n         [ 0.0152, -0.0788,  0.0157, -0.0108, -0.1813,  0.1412,  0.1137,\n           0.0492,  0.0142,  0.0539, -0.0273,  0.0068,  0.0354, -0.0185,\n           0.0092,  0.0373, -0.0081, -0.0709, -0.0727, -0.0787, -0.0217,\n          -0.0180, -0.0069, -0.0097,  0.0812,  0.0365, -0.0077,  0.0630,\n          -0.0269, -0.0525, -0.0700,  0.0088],\n         [ 0.0137, -0.0804,  0.0176, -0.0112, -0.1804,  0.1427,  0.1115,\n           0.0494,  0.0136,  0.0539, -0.0265,  0.0078,  0.0379, -0.0202,\n           0.0064,  0.0374, -0.0080, -0.0713, -0.0701, -0.0791, -0.0208,\n          -0.0181, -0.0074, -0.0091,  0.0809,  0.0363, -0.0140,  0.0597,\n          -0.0268, -0.0513, -0.0693,  0.0110],\n         [ 0.0147, -0.0797,  0.0167, -0.0111, -0.1808,  0.1422,  0.1124,\n           0.0490,  0.0139,  0.0539, -0.0268,  0.0074,  0.0369, -0.0195,\n           0.0075,  0.0374, -0.0081, -0.0712, -0.0716, -0.0787, -0.0212,\n          -0.0180, -0.0072, -0.0093,  0.0808,  0.0365, -0.0113,  0.0610,\n          -0.0269, -0.0518, -0.0693,  0.0100],\n         [ 0.0180, -0.0766,  0.0128, -0.0104, -0.1827,  0.1391,  0.1169,\n           0.0483,  0.0151,  0.0538, -0.0284,  0.0054,  0.0321, -0.0163,\n           0.0130,  0.0372, -0.0083, -0.0706, -0.0771, -0.0776, -0.0229,\n          -0.0175, -0.0063, -0.0105,  0.0811,  0.0369,  0.0012,  0.0671,\n          -0.0272, -0.0540, -0.0703,  0.0055],\n         [ 0.0213, -0.0731,  0.0088, -0.0098, -0.1843,  0.1356,  0.1213,\n           0.0475,  0.0163,  0.0537, -0.0302,  0.0034,  0.0269, -0.0129,\n           0.0188,  0.0371, -0.0086, -0.0698, -0.0828, -0.0764, -0.0246,\n          -0.0170, -0.0053, -0.0117,  0.0814,  0.0373,  0.0143,  0.0734,\n          -0.0276, -0.0563, -0.0714,  0.0009]]], device='cuda:0',\n       grad_fn=<NativeLayerNormBackward>)\ndec_state [tensor([[[ 0.1969,  0.1436, -0.0772,  0.0018, -0.0761, -0.0181,  0.1787,\n          -0.0380,  0.0319,  0.0525, -0.0316, -0.0139, -0.0655,  0.0068,\n           0.1188,  0.0563, -0.0451, -0.0943, -0.3334,  0.1194, -0.0459,\n           0.0010, -0.0121, -0.0086, -0.0243,  0.0828,  0.2981,  0.1206,\n          -0.0396, -0.0989, -0.0042, -0.0949],\n         [ 0.1498,  0.0338, -0.0677, -0.0004, -0.1021,  0.0237,  0.1708,\n          -0.0302,  0.0287,  0.0527, -0.0496, -0.0226, -0.0528,  0.0102,\n           0.1071,  0.0491, -0.0330, -0.0859, -0.2743,  0.0435, -0.0233,\n           0.0030,  0.0099, -0.0177,  0.0058,  0.0756,  0.2667,  0.1455,\n          -0.0350, -0.0856, -0.0374, -0.0927],\n         [ 0.1842,  0.1141, -0.0724,  0.0030, -0.0875, -0.0235,  0.1857,\n          -0.0254,  0.0351,  0.0523, -0.0399, -0.0177, -0.0716,  0.0188,\n           0.1209,  0.0540, -0.0406, -0.0921, -0.2964,  0.0906, -0.0504,\n          -0.0010, -0.0097, -0.0137, -0.0018,  0.0815,  0.2924,  0.1352,\n          -0.0397, -0.1003, -0.0247, -0.0963],\n         [ 0.1186,  0.0026, -0.0548, -0.0019, -0.1288,  0.0450,  0.1726,\n          -0.0112,  0.0318,  0.0527, -0.0537, -0.0211, -0.0469,  0.0188,\n           0.0927,  0.0484, -0.0278, -0.0841, -0.2237,  0.0056, -0.0311,\n           0.0059,  0.0131, -0.0196,  0.0433,  0.0692,  0.2218,  0.1435,\n          -0.0349, -0.0835, -0.0613, -0.0834],\n         [ 0.1811,  0.1305, -0.0720,  0.0037, -0.0814, -0.0175,  0.1828,\n          -0.0278,  0.0345,  0.0524, -0.0386, -0.0164, -0.0701,  0.0153,\n           0.1193,  0.0542, -0.0420, -0.0921, -0.3075,  0.0975, -0.0491,\n          -0.0007, -0.0098, -0.0117, -0.0088,  0.0813,  0.2895,  0.1253,\n          -0.0407, -0.0995, -0.0196, -0.0947],\n         [ 0.1815,  0.1183, -0.0709,  0.0033, -0.0933, -0.0294,  0.1879,\n          -0.0221,  0.0361,  0.0522, -0.0416, -0.0181, -0.0749,  0.0216,\n           0.1229,  0.0532, -0.0405, -0.0860, -0.2905,  0.0866, -0.0505,\n          -0.0015, -0.0091, -0.0146,  0.0011,  0.0815,  0.2933,  0.1344,\n          -0.0406, -0.1016, -0.0262, -0.0981],\n         [ 0.0085, -0.0508,  0.0771, -0.0287, -0.0619,  0.1472,  0.0590,\n           0.0087,  0.0132,  0.0543,  0.0228,  0.0525,  0.1699, -0.0931,\n          -0.0461,  0.0709, -0.0215, -0.1456, -0.0837, -0.0194,  0.1083,\n           0.0187, -0.0065,  0.0126, -0.0830,  0.0635, -0.1613, -0.0177,\n          -0.0338, -0.0112,  0.0178,  0.0279],\n         [ 0.0215, -0.0888,  0.0315, -0.0158, -0.1427,  0.1554,  0.0900,\n           0.0274,  0.0100,  0.0543, -0.0109,  0.0204,  0.0770, -0.0485,\n          -0.0227,  0.0464, -0.0087, -0.0959, -0.0692, -0.0517,  0.0027,\n          -0.0030, -0.0107,  0.0005,  0.0500,  0.0450, -0.0654,  0.0298,\n          -0.0288, -0.0351, -0.0468,  0.0251],\n         [ 0.0145, -0.0796,  0.0168, -0.0112, -0.1801,  0.1423,  0.1122,\n           0.0490,  0.0138,  0.0539, -0.0267,  0.0074,  0.0373, -0.0197,\n           0.0072,  0.0374, -0.0081, -0.0714, -0.0717, -0.0787, -0.0211,\n          -0.0179, -0.0073, -0.0092,  0.0807,  0.0364, -0.0118,  0.0609,\n          -0.0269, -0.0517, -0.0693,  0.0102],\n         [ 0.0186, -0.0755,  0.0117, -0.0101, -0.1828,  0.1380,  0.1182,\n           0.0484,  0.0154,  0.0538, -0.0290,  0.0048,  0.0304, -0.0152,\n           0.0149,  0.0371, -0.0084, -0.0702, -0.0782, -0.0776, -0.0235,\n          -0.0175, -0.0060, -0.0109,  0.0815,  0.0369,  0.0051,  0.0693,\n          -0.0273, -0.0547, -0.0710,  0.0042],\n         [ 0.0176, -0.0764,  0.0128, -0.0103, -0.1826,  0.1388,  0.1170,\n           0.0487,  0.0151,  0.0538, -0.0285,  0.0053,  0.0318, -0.0160,\n           0.0134,  0.0372, -0.0083, -0.0704, -0.0767, -0.0779, -0.0230,\n          -0.0177, -0.0062, -0.0106,  0.0815,  0.0368,  0.0017,  0.0676,\n          -0.0272, -0.0542, -0.0708,  0.0054],\n         [ 0.0152, -0.0788,  0.0157, -0.0108, -0.1813,  0.1412,  0.1137,\n           0.0492,  0.0142,  0.0539, -0.0273,  0.0068,  0.0354, -0.0185,\n           0.0092,  0.0373, -0.0081, -0.0709, -0.0727, -0.0787, -0.0217,\n          -0.0180, -0.0069, -0.0097,  0.0812,  0.0365, -0.0077,  0.0630,\n          -0.0269, -0.0525, -0.0700,  0.0088],\n         [ 0.0137, -0.0804,  0.0176, -0.0112, -0.1804,  0.1427,  0.1115,\n           0.0494,  0.0136,  0.0539, -0.0265,  0.0078,  0.0379, -0.0202,\n           0.0064,  0.0374, -0.0080, -0.0713, -0.0701, -0.0791, -0.0208,\n          -0.0181, -0.0074, -0.0091,  0.0809,  0.0363, -0.0140,  0.0597,\n          -0.0268, -0.0513, -0.0693,  0.0110],\n         [ 0.0147, -0.0797,  0.0167, -0.0111, -0.1808,  0.1422,  0.1124,\n           0.0490,  0.0139,  0.0539, -0.0268,  0.0074,  0.0369, -0.0195,\n           0.0075,  0.0374, -0.0081, -0.0712, -0.0716, -0.0787, -0.0212,\n          -0.0180, -0.0072, -0.0093,  0.0808,  0.0365, -0.0113,  0.0610,\n          -0.0269, -0.0518, -0.0693,  0.0100],\n         [ 0.0180, -0.0766,  0.0128, -0.0104, -0.1827,  0.1391,  0.1169,\n           0.0483,  0.0151,  0.0538, -0.0284,  0.0054,  0.0321, -0.0163,\n           0.0130,  0.0372, -0.0083, -0.0706, -0.0771, -0.0776, -0.0229,\n          -0.0175, -0.0063, -0.0105,  0.0811,  0.0369,  0.0012,  0.0671,\n          -0.0272, -0.0540, -0.0703,  0.0055],\n         [ 0.0213, -0.0731,  0.0088, -0.0098, -0.1843,  0.1356,  0.1213,\n           0.0475,  0.0163,  0.0537, -0.0302,  0.0034,  0.0269, -0.0129,\n           0.0188,  0.0371, -0.0086, -0.0698, -0.0828, -0.0764, -0.0246,\n          -0.0170, -0.0053, -0.0117,  0.0814,  0.0373,  0.0143,  0.0734,\n          -0.0276, -0.0563, -0.0714,  0.0009]]], device='cuda:0',\n       grad_fn=<NativeLayerNormBackward>), tensor([7], device='cuda:0'), [None, None]]\nY tensor([[[-24.1504, -24.1657, -24.1663,  -3.2576,  -0.7291,  -0.3629,  -5.1236,\n           -6.6616,  -1.8426,  -0.7919]]], device='cuda:0',\n       grad_fn=<SliceBackward>) tensor([[5]], device='cuda:0') torch.return_types.max(\nvalues=tensor([[-0.3629]], device='cuda:0', grad_fn=<MaxBackward0>),\nindices=tensor([[5]], device='cuda:0'))\nY tensor([[[-19.4554, -19.4546, -19.4749,   1.8437,   2.0362,   1.7064,   3.9747,\n            0.4900,   5.6404,  -2.0813]]], device='cuda:0',\n       grad_fn=<SliceBackward>) tensor([[8]], device='cuda:0') torch.return_types.max(\nvalues=tensor([[5.6404]], device='cuda:0', grad_fn=<MaxBackward0>),\nindices=tensor([[8]], device='cuda:0'))\nY tensor([[[-22.7436, -22.7546, -22.7411,  -1.3032,   0.6341,  -0.4800,  -2.4212,\n           -4.8728,  -2.9418,  -4.9095]]], device='cuda:0',\n       grad_fn=<SliceBackward>) tensor([[4]], device='cuda:0') torch.return_types.max(\nvalues=tensor([[0.6341]], device='cuda:0', grad_fn=<MaxBackward0>),\nindices=tensor([[4]], device='cuda:0'))\nY tensor([[[-21.6481, -21.6809, -21.6981,   1.5951,  -0.5749,  -0.9619,   3.1314,\n           -1.1307,   1.9265,  -3.2942]]], device='cuda:0',\n       grad_fn=<SliceBackward>) tensor([[6]], device='cuda:0') torch.return_types.max(\nvalues=tensor([[3.1314]], device='cuda:0', grad_fn=<MaxBackward0>),\nindices=tensor([[6]], device='cuda:0'))\nY tensor([[[-24.0010, -23.9898, -24.0103,   0.6137,  -2.0477,  -1.6432,  -3.1603,\n           -4.1925,  -1.9093,  -4.4188]]], device='cuda:0',\n       grad_fn=<SliceBackward>) tensor([[3]], device='cuda:0') torch.return_types.max(\nvalues=tensor([[0.6137]], device='cuda:0', grad_fn=<MaxBackward0>),\nindices=tensor([[3]], device='cuda:0'))\n[5, 8, 4, 6]\n['我', '是', '你', '的']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'我 是 你 的'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "predict(\"你叫什么名字\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "torch.arange(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 1, 2, 3, 4],\n",
       "        [1, 2, 3, 4, 1, 2, 3, 4],\n",
       "        [1, 2, 3, 4, 1, 2, 3, 4],\n",
       "        [1, 2, 3, 4, 1, 2, 3, 4]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "torch.arange(1, 5).repeat(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import torch\n",
    "torch.ones(1, 4, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2., 2.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "torch.ones(2, 4, 6) + torch.ones(1, 4, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}